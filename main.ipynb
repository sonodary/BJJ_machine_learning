{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data file\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "### Read flattened, processed data\n",
    "import pandas as pd\n",
    "annotation = pd.read_csv(\"data/annotations.csv\")\n",
    "\n",
    "## Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_fash, test_fash = train_test_split(fash_mnist, test_size=0.1, random_state=5)\n",
    "\n",
    "### Separate the label column (outcome)\n",
    "# train_y = train_fash['y']\n",
    "# train_X = train_fash.drop(['y'], axis=1)\n",
    "# test_y = test_fash['y']\n",
    "# test_X = test_fash.drop(['y'], axis=1)\n",
    "\n",
    "### Convert to numpy array then reshape to 900 by 28 by 28\n",
    "# mnist_unflattened = train_X.to_numpy()\n",
    "# mnist_unflattened = mnist_unflattened.reshape(900,28,28)\n",
    "\n",
    "## Convert to tensor\n",
    "# mnist_tensor = torch.from_numpy(mnist_unflattened)\n",
    "\n",
    "## Check shape of the first image\n",
    "# print(mnist_tensor[0,:,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation.head()\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The size is 64 x 64 now\n",
    "\n",
    "from skimage import io\n",
    "path = 'batch1/part_1/'\n",
    "for item in os.listdir(path):\n",
    "        if os.path.isfile(path+item):\n",
    "            if(path+item == 'batch1/part_1/.DS_Store'):\n",
    "                continue\n",
    "            im = Image.open(path+item)\n",
    "            f, e = os.path.splitext(path+item)\n",
    "            imResize = im.resize((64,64), Image.Resampling.LANCZOS)\n",
    "            imResize.save(path + 'new/' + item, 'JPEG')\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = 'batch1/part_1/new/'\n",
    "img_names = os.listdir(path)\n",
    "\n",
    "images = np.empty(shape = (10025, 64, 64, 3))\n",
    "for idx, name in enumerate(img_names):\n",
    "    img_name = path + name\n",
    "    # Use you favourite library to load the image\n",
    "    image = plt.imread(img_name)\n",
    "    images[idx] = image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rv/qhv_6w4n707gq2vr6xwbmj9w0000gn/T/ipykernel_65150/4127807505.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  annotation['position_num'][i] = position_maps[annotation[\"position\"][i]]\n"
     ]
    }
   ],
   "source": [
    "# print(annotation_batch1.head())\n",
    "import pandas as pd\n",
    "\n",
    "position_maps= {\"standing\": 0, \n",
    "                \"takedown1\": 1,\n",
    "                \"takedown2\": 2,\n",
    "                \"open_guard1\": 3,\n",
    "                \"open_guard2\": 4,\n",
    "                \"half_guard1\": 5,\n",
    "                \"half_guard2\": 6,\n",
    "                \"closed_guard1\": 7,\n",
    "                \"closed_guard2\": 8,\n",
    "                \"5050_guard\": 9,\n",
    "                \"mount1\": 10,\n",
    "                \"mount2\": 11,\n",
    "                \"back1\": 12,\n",
    "                \"back2\": 13,\n",
    "                \"turtle1\": 14,\n",
    "                \"turtle2\": 15,               \n",
    "                \"side_control1\" : 16,\n",
    "                \"side_control2\" : 17}\n",
    "\n",
    "\n",
    "annotation['position_num'] = [0]*len(annotation['position'])\n",
    "for i in range(10025):\n",
    "#     print(annotation[\"position\"][i])\n",
    "    annotation['position_num'][i] = position_maps[annotation[\"position\"][i]]\n",
    "annotation_batch1 = annotation['position_num'][:10025]\n",
    "\n",
    "\n",
    "annotation_batch1 = torch.from_numpy(annotation_batch1.values)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    images, annotation_batch1, test_size=0.25, random_state=42)\n",
    "\n",
    "train_X = torch.from_numpy(train_X)\n",
    "train_X = torch.movedim(train_X, source=3, destination=1)\n",
    "\n",
    "test_X = torch.from_numpy(test_X)\n",
    "test_X = torch.movedim(test_X, source=3, destination=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compose Transformations\n",
    "from torchvision import transforms\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.GaussianBlur(kernel_size=(3,3), sigma=(0.1, 5)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=(0, 180))\n",
    "])\n",
    "\n",
    "from torch import nn\n",
    "class my_net(nn.Module):\n",
    "    \n",
    "    ## Constructor commands\n",
    "    def __init__(self):\n",
    "        super(my_net, self).__init__()\n",
    "        \n",
    "        ## Define architecture\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(3,8,3,1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(8,16,2,1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3600, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 18)\n",
    "        )\n",
    "    \n",
    "    ## Function to generate predictions\n",
    "    def forward(self, x):\n",
    "        scores = self.conv_stack(x)\n",
    "        return scores\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rv/qhv_6w4n707gq2vr6xwbmj9w0000gn/T/ipykernel_65150/1651398758.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mcur_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Part D\n",
    "## Hyperparms\n",
    "epochs = 300\n",
    "lrate = 0.001\n",
    "bsize = 28\n",
    "\n",
    "## For reproduction purposes \n",
    "torch.manual_seed(3)\n",
    "\n",
    "## Cost Function\n",
    "cost_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "## Intialize the model\n",
    "net = my_net()\n",
    "\n",
    "## Optimizer (Stochastic Gradient Descent)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lrate)\n",
    "\n",
    "\n",
    "\n",
    "## Make DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "y_tensor = torch.Tensor(train_y)\n",
    "train_loader = DataLoader(TensorDataset(train_X.type(torch.FloatTensor), \n",
    "                        y_tensor.type(torch.LongTensor)), batch_size=bsize)\n",
    "\n",
    "## Re-run the training loop, notice the new data_transforms() command\n",
    "track_cost = np.zeros(epochs)\n",
    "cur_cost = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cur_cost = 0.0\n",
    "    correct = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        ## Transform the input data using our data augmentation strategies\n",
    "        inputs = data_transforms(inputs)\n",
    "        \n",
    "        ## Same as before\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        cost = cost_fn(nn.Softmax(dim=1)(outputs), labels)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        cur_cost += cost.item()\n",
    "    \n",
    "    ## Store the accumulated cost at each epoch\n",
    "    track_cost[epoch] = cur_cost\n",
    "#     print(f\"Epoch: {epoch} Cost: {cur_cost}\") ## Uncomment this if you want printed updates\n",
    "    \n",
    "\n",
    "## Initialize objects for counting correct/total \n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Specify no changes to the gradient in the subsequent steps (since we're not using these data for training)\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        # Current batch of data\n",
    "        images, labels = data\n",
    "        \n",
    "        # pass each batch into the network\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # the class with the maximum score is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # add size of the current batch\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # add the number of correct predictions in the current batch\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "## Calculate and print the proportion correct\n",
    "print(f\"Training Accuracy is {correct/total}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
