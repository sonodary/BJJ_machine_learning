{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Wrangling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in data using the functions that we made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneBatch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1BDwQ8VGLXKCv7GWSKGlCtfFs-kCokwGW\n",
      "From (redirected): https://drive.google.com/uc?id=1BDwQ8VGLXKCv7GWSKGlCtfFs-kCokwGW&confirm=t&uuid=28010d40-1ae1-491a-ab4f-b7ef4080612f\n",
      "To: /Users/sonodaryuuichirou/Documents/GitHub/Jiujustsu_machine_learning/batch2.zip\n",
      " 14%|█████▎                                 | 91.2M/667M [00:11<01:24, 6.83MB/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# batchNums = list(range(1,13))\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     batchNums \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m---> 15\u001b[0m     ids, images \u001b[38;5;241m=\u001b[39m \u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadImageBatches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatchNums\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Jiujustsu_machine_learning/data_loading.py:178\u001b[0m, in \u001b[0;36mloadImageBatches\u001b[0;34m(nums)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num \u001b[38;5;129;01min\u001b[39;00m nums:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mprint\u001b[39m(num)\n\u001b[0;32m--> 178\u001b[0m     ids, images \u001b[38;5;241m=\u001b[39m \u001b[43mloadImageBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# Adding to arrays\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     idArrays\u001b[38;5;241m.\u001b[39mappend(ids)\n",
      "File \u001b[0;32m~/Documents/GitHub/Jiujustsu_machine_learning/data_loading.py:156\u001b[0m, in \u001b[0;36mloadImageBatch\u001b[0;34m(num)\u001b[0m\n\u001b[1;32m    154\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./batch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# We download and unzip batch from drive\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m \u001b[43mgetBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munzip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# We import images into np.arrays\u001b[39;00m\n\u001b[1;32m    159\u001b[0m newPath \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/part_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Jiujustsu_machine_learning/data_loading.py:53\u001b[0m, in \u001b[0;36mgetBatch\u001b[0;34m(num, path, unzip)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Requesting file from Google Drive\u001b[39;00m\n\u001b[1;32m     52\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m \u001b[43mgdown\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuzzy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Unzip\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unzip:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gdown/download.py:334\u001b[0m, in \u001b[0;36mdownload\u001b[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format)\u001b[0m\n\u001b[1;32m    332\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(total\u001b[38;5;241m=\u001b[39mtotal, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    333\u001b[0m t_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m res\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mCHUNK_SIZE):\n\u001b[1;32m    335\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/http/client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 465\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▎                                 | 91.8M/667M [00:28<01:24, 6.83MB/s]"
     ]
    }
   ],
   "source": [
    "# Run if you want one batch\n",
    "import data_loading as dt\n",
    "import os\n",
    "\n",
    "\n",
    "if oneBatch:\n",
    "    batchNum = 1\n",
    "    if os.path.exists(f'batch{batchNum}/part_{batchNum}/'):\n",
    "        ids, images = dt.importImages(f'batch{batchNum}/part_{batchNum}/')\n",
    "    else:\n",
    "        ids, images = dt.loadImageBatch(batchNum)\n",
    "else:\n",
    "    # batchNums = list(range(1,13))\n",
    "    batchNums = [2, 7, 10]\n",
    "    ids, images = dt.loadImageBatches(batchNums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'data_loading' has no attribute 'importImagesBatches'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata_loading\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ids, images \u001b[38;5;241m=\u001b[39m  \u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimportImagesBatches\u001b[49m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocuments/GitHub/Jiujustsu_machine_learning/batch2/part_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocuments/GitHub/Jiujustsu_machine_learning/batch7/part_7\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocuments/GitHub/Jiujustsu_machine_learning/batch10/part_10\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'data_loading' has no attribute 'importImagesBatches'"
     ]
    }
   ],
   "source": [
    "import data_loading as dt\n",
    "ids, images =  dt.importImagesBatches([\"Documents/GitHub/Jiujustsu_machine_learning/batch2/part_2\", \"Documents/GitHub/Jiujustsu_machine_learning/batch7/part_7\", \"Documents/GitHub/Jiujustsu_machine_learning/batch10/part_10\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting annotations and getting them into the correct order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>standing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standing</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>standing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>standing</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>standing</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   position  image\n",
       "0  standing      1\n",
       "1  standing      2\n",
       "2  standing      3\n",
       "3  standing      4\n",
       "4  standing      5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "annotations = pd.read_csv(\"data/annotations.csv\")\n",
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 22\u001b[0m\n\u001b[1;32m      1\u001b[0m position_maps\u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstanding\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \n\u001b[1;32m      2\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtakedown1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      3\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtakedown2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mside_control1\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     18\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mside_control2\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m17\u001b[39m}\n\u001b[1;32m     21\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mids\u001b[49m:\n\u001b[1;32m     23\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(position_maps[annotations[annotations[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mid\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ids' is not defined"
     ]
    }
   ],
   "source": [
    "position_maps= {\"standing\": 0, \n",
    "                \"takedown1\": 1,\n",
    "                \"takedown2\": 2,\n",
    "                \"open_guard1\": 3,\n",
    "                \"open_guard2\": 4,\n",
    "                \"half_guard1\": 5,\n",
    "                \"half_guard2\": 6,\n",
    "                \"closed_guard1\": 7,\n",
    "                \"closed_guard2\": 8,\n",
    "                \"5050_guard\": 9,\n",
    "                \"mount1\": 10,\n",
    "                \"mount2\": 11,\n",
    "                \"back1\": 12,\n",
    "                \"back2\": 13,\n",
    "                \"turtle1\": 14,\n",
    "                \"turtle2\": 15,               \n",
    "                \"side_control1\" : 16,\n",
    "                \"side_control2\" : 17}\n",
    "\n",
    "\n",
    "labels = []\n",
    "for id in ids:\n",
    "    labels.append(position_maps[annotations[annotations['image'] == id]['position'].reset_index(drop=True)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting images to tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Converting into torch tensors\n",
    "for i, img in enumerate(images):\n",
    "    images[i] = torch.from_numpy(np.array(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data argumentation\n",
    "from torchvision import transforms\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.GaussianBlur(kernel_size=(3,3), sigma=(0.1, 5)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=(0, 180))\n",
    "])\n",
    "\n",
    "# CNN\n",
    "from torch import nn\n",
    "import torchvision\n",
    "class my_net(nn.Module):\n",
    "    \n",
    "    ## Constructor commands\n",
    "    def __init__(self):\n",
    "        super(my_net, self).__init__()\n",
    "        \n",
    "        ## Define architecture\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(3,8,3,1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(8,16,2,1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(16,32,3,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7600, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 18)\n",
    "        )\n",
    "    \n",
    "    ## Function to generate predictions\n",
    "    def forward(self, x):\n",
    "        scores = self.conv_stack(x)\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    images, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "train_X = torch.from_numpy(train_X)\n",
    "train_X = torch.movedim(train_X, source=3, destination=1)\n",
    "\n",
    "test_X = torch.from_numpy(test_X)\n",
    "test_X = torch.movedim(test_X, source=3, destination=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 10\n",
    "lrate = 0.001\n",
    "bsize = 256\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(3)\n",
    "\n",
    "# Cost Function\n",
    "cost_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the model\n",
    "net = my_net()\n",
    "\n",
    "# Optimizer (Stochastic Gradient Descent)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lrate)\n",
    "\n",
    "# Make DataLoader\n",
    "y_tensor = torch.Tensor(train_y)\n",
    "train_loader = DataLoader(TensorDataset(train_X.type(torch.FloatTensor),\n",
    "                                        y_tensor.type(torch.LongTensor)), batch_size=bsize)\n",
    "\n",
    "# Training loop\n",
    "track_cost = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cur_cost = 0.0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Transform the input data using our data augmentation strategies\n",
    "        inputs = data_transforms(inputs)\n",
    "\n",
    "        # Forward, backward, and optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        cost = cost_fn(outputs, labels)  # CrossEntropyLoss already applies Softmax\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cur_cost += cost.item()\n",
    "\n",
    "    # Store the accumulated cost at each epoch\n",
    "    track_cost[epoch] = cur_cost\n",
    "    print(epoch/epochs)\n",
    "    #print(f\"Epoch: {epoch} Cost: {cur_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating training accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize objects for counting correct/total \n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Specify no changes to the gradient in the subsequent steps (since we're not using these data for training)\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        # Current batch of data\n",
    "        images, labels = data\n",
    "        \n",
    "        # pass each batch into the network\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # the class with the maximum score is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # add size of the current batch\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # add the number of correct predictions in the current batch\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "## Calculate and print the proportion correct\n",
    "print(f\"Training Accuracy is {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating testing accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Combine X and y tensors into a TensorDataset and DataLoader\n",
    "test_loader = DataLoader(TensorDataset(test_X.type(torch.FloatTensor), \n",
    "                        torch.Tensor(test_y).type(torch.LongTensor)), batch_size=bsize)\n",
    "\n",
    "## Initialize objects for counting correct/total \n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Specify no changes to the gradient in the subsequent steps (since we're not using these data for training)\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        # Current batch of data\n",
    "        images, labels = data\n",
    "        \n",
    "        # pass each batch into the network\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # the class with the maximum score is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # add size of the current batch\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # add the number of correct predictions in the current batch\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "## Calculate and print the proportion correct\n",
    "print(f\"Test Accuracy is {correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|███████████▊                            | 210M/714M [01:36<03:43, 2.26MB/s]"
     ]
    }
   ],
   "source": [
    "# verifying the convergence of cost\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.linspace(0, epochs, epochs), track_cost)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
